# ğŸ§  DL Assignment-1 â€” Feedforward Neural Network from Scratch

This repository contains an implementation of a modular **Feedforward Neural Network (FFNN)** built from scratch using only NumPy. The model is trained on both **Fashion MNIST** and **MNIST** datasets and supports experimentation with multiple optimizers, loss functions, weight initialization strategies, and activation functions. Hyperparameter tuning is performed using **Weights & Biases (W&B)** sweeps.

---

## ğŸ“ Folder Structure

```
DL ASSIGNMENT-1/
â”œâ”€â”€ Model1.py               # Main model class: forward, backward, training loop
â”œâ”€â”€ Optimizer.py            # SGD, Momentum, NAG, RMSprop, Adam, Nadam
â”œâ”€â”€ Helper_Functions.py     # Gradient updates and optimizer helper functions
â”œâ”€â”€ Utils.py                # Activation/loss functions, weight init, data utils
â”œâ”€â”€ train.py                # Command-line train script
â”œâ”€â”€ sweep_train.py          # Wandb sweep agent script
â”œâ”€â”€ sweep_config.yaml       # YAML config for wandb sweeps
â”œâ”€â”€ compare.py              # Compare MSE vs Cross Entropy loss
â”œâ”€â”€ best_run.py             # Script to load and evaluate saved model
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ tests/                  # Unit tests for utility functions
â”‚   â”œâ”€â”€ test_utils.py
â”‚   â”œâ”€â”€ test_helper.py
â”‚   â””â”€â”€ test_optimizer.py
â”œâ”€â”€ model_final.p           # Serialized trained model (pickle)
â”œâ”€â”€ question_1.py           # Q1 logic (part of assignment)
â”œâ”€â”€ .gitignore              # Git ignore rules
â””â”€â”€ README.md               # â† You're here!
```

---

## âš™ï¸ Setup Instructions

### ğŸ”¹ Step 1: Clone and Set Up Virtual Environment

```bash
git clone https://github.com/<your-id>/da6401_assignment1
cd da6401_assignment1

# Create and activate virtual environment
python -m venv venv
venv\Scripts\activate     # On Windows

# Install all dependencies
pip install -r requirements.txt
```

---

## ğŸš€ Training the Model with Custom Parameters

Train your model using `train.py` with command-line arguments for full control.

### âœ… Example Command:

```bash
python train.py \
  --loss cross_entropy \
  --optimizer adam \
  --epochs 30 \
  --batch_size 64 \
  --activation ReLu \
  --num_layers 3 \
  --learning_rate 0.001
```

### âš™ï¸ Supported Hyperparameters

| Flag             | Values                                                                 |
|------------------|------------------------------------------------------------------------|
| `--loss`         | `cross_entropy`, `mean_squared_error`                                  |
| `--optimizer`    | `sgd`, `momentum`, `nag`, `rmsprop`, `adam`, `nadam`                   |
| `--activation`   | `ReLu`, `tanh`, `sigmoid`                                               |
| `--num_layers`   | 2, 3, 4, 5                                                              |
| `--batch_size`   | 16, 32, 64, 128                                                         |
| `--learning_rate`| e.g., 0.001, 0.0001                                                     |
| `--epochs`       | e.g., 5, 10, 30, 40                                                     |

---

## ğŸ“Š Hyperparameter Tuning via W&B Sweeps

To run a sweep using your own W&B credentials:

```bash
python sweep_train.py \
  --entity <your-wandb-username> \
  --project da24m019_shreya_da6401_assignment1 \
  --count 10
```

Edit `sweep_config.yaml` to modify parameter ranges or strategy (e.g., `grid`, `random`, `bayes`).

---

## ğŸ“ˆ Compare Loss Functions: MSE vs Cross Entropy

Run this script to generate performance plots comparing **mean squared error** and **cross entropy**:

```bash
python compare.py
```

Plots will be logged to W&B automatically and also printed to console.

---

## âœ… Running Unit Tests

Run unit tests for all utility and optimizer functions:

```bash
pytest tests/
```

---

## ğŸ§ª Best Run Example

```text
Run Name: hl_4_bs_32_ac_ReLu_opt_nadam_lr_0.00071
Epochs: 10
Number of Hidden Layers: 4
Hidden Layer Size: 64
Weight Decay: 0.0005
Learning Rate: 0.00071
Optimizer: Nadam
Batch Size: 32
Weight Initialization: Xavier
Activation Function: ReLU
```

---

## âœ… Notes for Evaluator

- ğŸ”’ **No Test Leakage**: Test set has not been used in training or validation.
- âœ… **Proper Splitting**: 10% of the training set is used as validation.
- ğŸ§¹ **Clean Code**: All helper logic is modularized. Code passes `pytest` tests.
- ğŸ” **Reproducible**: Set seeds for shuffling and W&B logs track hyperparameters.
- ğŸ“Š **Plots**: All plots (validation loss, accuracy, comparison) are added to the report and W&B.
- âœï¸ **Original**: Code is completely original. No copied code or plagiarism.

---

## ğŸ“ GitHub Submission Link

```
https://github.com/<your-id>/da6401_assignment1
```

Please replace `<your-id>` with your GitHub username.

---

## ğŸ“¦ To Regenerate `requirements.txt`

If you modify or add packages, regenerate the requirements file with:

```bash
pip freeze > requirements.txt
```

---

Made with â¤ï¸ for **DA6401: Deep Learning Assignment-1**